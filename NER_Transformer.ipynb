{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0dynsFSQIPe"
      },
      "source": [
        "## Training Albert Transformer For Named Entity Recognition\n",
        "\n",
        "This colab notebook shows how to train an Albert Transformer (albert base v1) on NER downstream task by using the 'run_ner.py' script from [Transformers library](https://github.com/huggingface/transformers). The Conll dataset is chosen as the benchmark dataset for this purpose, particularly the German Conll dataset. The steps are as follows:\n",
        "\n",
        "- Install Transformers from source\n",
        "- Extract and run the run ner and preprocess scripts\n",
        "- Download the Conllu dataset\n",
        "- Run the script:\n",
        "```python\n",
        "!python transformers-3.4.0/examples/token-classification/run_tf_ner.py \\\n",
        "  --data_dir ./ \\\n",
        "  --labels ./labels.txt \\\n",
        "  --model_name_or_path $MODEL \\\n",
        "  --output_dir $OUTPUT_DIR \\\n",
        "  --max_seq_length  $MAX_LENGTH \\\n",
        "  --num_train_epochs $NUM_EPOCHS \\\n",
        "  --per_gpu_train_batch_size $BATCH_SIZE \\\n",
        "  --save_steps $SAVE_STEPS \\\n",
        "  --logging_steps $LOGGING_STEPS \\\n",
        "  --seed $SEED \\\n",
        "  --do_train \\\n",
        "  --do_predict \\\n",
        "  --overwrite_output_dir\n",
        "  ```\n",
        "\n",
        "  - Specify the hyperparameters of the albert model including the epochs, training steps etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3MScuXQYfsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c84632-b14a-4d2c-fbf8-fe2a16f4ba4c"
      },
      "source": [
        "!wget 'https://github.com/huggingface/transformers/archive/v3.4.0.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 11:57:43--  https://github.com/huggingface/transformers/archive/v3.4.0.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/huggingface/transformers/zip/v3.4.0 [following]\n",
            "--2021-03-07 11:57:43--  https://codeload.github.com/huggingface/transformers/zip/v3.4.0\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘v3.4.0.zip.1’\n",
            "\n",
            "v3.4.0.zip.1            [    <=>             ]   7.21M  8.86MB/s    in 0.8s    \n",
            "\n",
            "2021-03-07 11:57:44 (8.86 MB/s) - ‘v3.4.0.zip.1’ saved [7563073]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9f_LQo9YnuL"
      },
      "source": [
        "!unzip 'v3.4.0.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DpSllpVY4ZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff978e5-3fc7-4941-a0c1-05d505630699"
      },
      "source": [
        "!curl -L 'https://drive.google.com/uc?export=download&id=1Jjhbal535VVz2ap4v4r_rN1UEHTdLK5P' \\\n",
        "| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > train.txt.tmp\n",
        "!curl -L 'https://drive.google.com/uc?export=download&id=1ZfRcQThdtAR5PPRjIDtrVP7BtXSCUBbm' \\\n",
        "| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' A> dev.txt.tmp\n",
        "!curl -L 'https://drive.google.com/uc?export=download&id=1u9mb7kNJHWQCWyweMDRMuTFoOHOfeBTH' \\\n",
        "| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > test.txt.tmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   388    0   388    0     0    287      0 --:--:--  0:00:01 --:--:--   287\n",
            "100 7697k    0 7697k    0     0  4736k      0 --:--:--  0:00:01 --:--:-- 4736k\n",
            "tr: extra operand ‘A’\n",
            "Try 'tr --help' for more information.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   388    0   388    0     0    317      0 --:--:--  0:00:01 --:--:--   317\n",
            "  7  706k    7 53802    0     0  40031      0  0:00:18  0:00:01  0:00:17 40031\n",
            "curl: (23) Failed writing body (717 != 1271)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   388    0   388    0     0    414      0 --:--:-- --:--:-- --:--:--   414\n",
            "100 1643k  100 1643k    0     0  1506k      0  0:00:01  0:00:01 --:--:-- 61.2M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILnFvUisZdNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de518109-e4f7-497e-98dc-b6f2d9c88f7f"
      },
      "source": [
        "!wc -l dev.txt.tmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 dev.txt.tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYrzVLXEZ-vy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e687c1-62e0-4d24-a30a-9af6179d8563"
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 11:58:04--  https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 991 [text/plain]\n",
            "Saving to: ‘preprocess.py.2’\n",
            "\n",
            "\rpreprocess.py.2       0%[                    ]       0  --.-KB/s               \rpreprocess.py.2     100%[===================>]     991  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-07 11:58:04 (77.8 MB/s) - ‘preprocess.py.2’ saved [991/991]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRFnooPEUcUm"
      },
      "source": [
        "MAX_LENGTH=128\n",
        "MODEL='albert-base-v1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR5OBYPMZt_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e890eedd-4325-423d-e17f-5becab920c2f"
      },
      "source": [
        "!python3 preprocess.py train.txt.tmp $MODEL $MAX_LENGTH > train.txt\n",
        "!python3 preprocess.py dev.txt.tmp $MODEL $MAX_LENGTH > dev.txt\n",
        "!python3 preprocess.py test.txt.tmp $MODEL $MAX_LENGTH > test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-07 11:58:10.406288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Downloading: 100% 684/684 [00:00<00:00, 504kB/s]\n",
            "Downloading: 100% 760k/760k [00:00<00:00, 10.6MB/s]\n",
            "2021-03-07 11:58:38.077608: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-07 11:58:42.912366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wPUllZZZGAO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW0XKgTqYuQz"
      },
      "source": [
        "cat train.txt dev.txt test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08OcyuZ0awCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70444137-9ee6-4cb3-dc5f-61a4562ff8b8"
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/huggingface/transformers/v3.4.0/examples/token-classification/run_ner.py'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 11:58:56--  https://raw.githubusercontent.com/huggingface/transformers/v3.4.0/examples/token-classification/run_ner.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11596 (11K) [text/plain]\n",
            "Saving to: ‘run_ner.py.1’\n",
            "\n",
            "\rrun_ner.py.1          0%[                    ]       0  --.-KB/s               \rrun_ner.py.1        100%[===================>]  11.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-07 11:58:56 (94.7 MB/s) - ‘run_ner.py.1’ saved [11596/11596]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn70ZxNGa9h9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b94f01-c3ac-45bb-aebb-52402c7ec1e5"
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/huggingface/transformers/v3.4.0/examples/token-classification/utils_ner.py' as 'utils_ner.py'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 11:58:56--  https://raw.githubusercontent.com/huggingface/transformers/v3.4.0/examples/token-classification/utils_ner.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15629 (15K) [text/plain]\n",
            "Saving to: ‘utils_ner.py.1’\n",
            "\n",
            "\rutils_ner.py.1        0%[                    ]       0  --.-KB/s               \rutils_ner.py.1      100%[===================>]  15.26K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-03-07 11:58:56 (29.2 MB/s) - ‘utils_ner.py.1’ saved [15629/15629]\n",
            "\n",
            "--2021-03-07 11:58:56--  http://as/\n",
            "Resolving as (as)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘as’\n",
            "--2021-03-07 11:58:56--  http://utils_ner.py/\n",
            "Resolving utils_ner.py (utils_ner.py)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘utils_ner.py’\n",
            "FINISHED --2021-03-07 11:58:56--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 15K in 0.001s (29.2 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rakTHqVaZldb"
      },
      "source": [
        "OUTPUT_DIR='abhilash1910/ner-model'\n",
        "BATCH_SIZE=32\n",
        "NUM_EPOCHS=3\n",
        "SAVE_STEPS=750\n",
        "SEED=1\n",
        "SAVE_STEPS = 100 #@param {type: \"integer\"}\n",
        "LOGGING_STEPS = 100 #@param {type: \"integer\"}\n",
        "SEED = 42 #@param {type: \"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u-FzPFCbmjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae40441d-57d9-4ffd-d27a-a456fa96adfc"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/73/742d17d8a9a1c639132affccc9250f0743e484cbf263ede6ddcbe34ef212/datasets-1.4.1-py3-none-any.whl (186kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 13.2MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 30kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 40kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 102kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 112kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 122kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 133kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 143kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 153kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 163kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 174kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 184kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Collecting huggingface-hub==0.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/b5/93/7cb0755c62c36cdadc70c79a95681df685b52cbaf76c724facb6ecac3272/huggingface_hub-0.0.2-py3-none-any.whl\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, huggingface-hub, fsspec, datasets\n",
            "Successfully installed datasets-1.4.1 fsspec-0.8.7 huggingface-hub-0.0.2 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UB27nFkbuYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c25b08-2a5c-484c-ca5a-40a0cab8f950"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZEse4-zbOG0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwnws__-drEW"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "%cd ..\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i26QjiSAVZED"
      },
      "source": [
        "!cd transformers/examples/token-classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk6-DGSSVN6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff62252-57ff-4652-84d6-315993fb2596"
      },
      "source": [
        "import glob\n",
        "f=glob.glob('transformers-3.4.0/examples/token-classification/*')\n",
        "f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['transformers-3.4.0/examples/token-classification/run.sh',\n",
              " 'transformers-3.4.0/examples/token-classification/utils_ner.py',\n",
              " 'transformers-3.4.0/examples/token-classification/run_ner.py',\n",
              " 'transformers-3.4.0/examples/token-classification/README.md',\n",
              " 'transformers-3.4.0/examples/token-classification/run_pos.sh',\n",
              " 'transformers-3.4.0/examples/token-classification/run_pos_pl.sh',\n",
              " 'transformers-3.4.0/examples/token-classification/run_pl.sh',\n",
              " 'transformers-3.4.0/examples/token-classification/run_chunk.sh',\n",
              " 'transformers-3.4.0/examples/token-classification/scripts',\n",
              " 'transformers-3.4.0/examples/token-classification/run_pl_ner.py',\n",
              " 'transformers-3.4.0/examples/token-classification/run_tf_ner.py',\n",
              " 'transformers-3.4.0/examples/token-classification/tasks.py',\n",
              " 'transformers-3.4.0/examples/token-classification/__pycache__',\n",
              " 'transformers-3.4.0/examples/token-classification/test_ner_examples.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po_1HigAT0Fg",
        "outputId": "a241e563-56dc-42ca-a645-902396e11780"
      },
      "source": [
        "!pip install conllu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting conllu\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/be/be6959c3ff2dbfdd87de4be0ccdff577835b5d08b1d25bf7fd4aaf0d7add/conllu-4.4-py2.py3-none-any.whl\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9pGk1u8V_wS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc56c76d-bd7e-446b-ce4d-8dce7d953ded"
      },
      "source": [
        "!pip install --upgrade pyarrow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pyarrow in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0_Pk7KtU8hR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AgFhttXYU8l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7967412f-b744-44d2-d095-0dd79fa200bd"
      },
      "source": [
        "f=open('./labels.txt').read()\n",
        "f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'B-LOC\\nB-LOCderiv\\nB-LOCpart\\nB-ORG\\nB-ORGderiv\\nB-ORGpart\\nB-OTH\\nB-OTHderiv\\nB-OTHpart\\nB-PER\\nB-PERderiv\\nB-PERpart\\nI-LOC\\nI-LOCderiv\\nI-LOCpart\\nI-ORG\\nI-ORGpart\\nI-OTH\\nI-OTHderiv\\nI-OTHpart\\nI-PER\\nI-PERderiv\\nI-PERpart\\nO\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2X0CbfhWu-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eea2cc0-31fc-4c32-db28-d9b1d1b9a15e"
      },
      "source": [
        "!python transformers-3.4.0/examples/token-classification/run_tf_ner.py \\\n",
        "  --data_dir ./ \\\n",
        "  --labels ./labels.txt \\\n",
        "  --model_name_or_path $MODEL \\\n",
        "  --output_dir $OUTPUT_DIR \\\n",
        "  --max_seq_length  $MAX_LENGTH \\\n",
        "  --num_train_epochs $NUM_EPOCHS \\\n",
        "  --per_gpu_train_batch_size $BATCH_SIZE \\\n",
        "  --save_steps $SAVE_STEPS \\\n",
        "  --logging_steps $LOGGING_STEPS \\\n",
        "  --seed $SEED \\\n",
        "  --do_train \\\n",
        "  --do_predict \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-07 11:59:29.439218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-07 11:59:31.722390: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-07 11:59:31.723505: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-07 11:59:31.734753: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-07 11:59:31.734812: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (546f1109f418): /proc/driver/nvidia/version does not exist\n",
            "2021-03-07 11:59:31.735598: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "03/07/2021 11:59:31 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n",
            "03/07/2021 11:59:31 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='abhilash1910/ner-model', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Mar07_11-59-31_546f1109f418', logging_first_step=False, logging_steps=100, save_steps=100, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='abhilash1910/ner-model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, tpu_name=None, poly_power=1.0, xla=False)\n",
            "03/07/2021 11:59:32 - INFO - filelock -   Lock 139846977989648 acquired on /root/.cache/torch/transformers/94ff9445d775ac7ad0c5f9bf37c17e9de49d0c1e0efcaf91991b222fa73c8718.33259199971252496b2ee0dca4f03c243cca9c787fb5846f62df14ecdc63720c.h5.lock\n",
            "Downloading: 100% 63.0M/63.0M [00:01<00:00, 50.8MB/s]\n",
            "03/07/2021 11:59:33 - INFO - filelock -   Lock 139846977989648 released on /root/.cache/torch/transformers/94ff9445d775ac7ad0c5f9bf37c17e9de49d0c1e0efcaf91991b222fa73c8718.33259199971252496b2ee0dca4f03c243cca9c787fb5846f62df14ecdc63720c.h5.lock\n",
            "2021-03-07 11:59:34.069706: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "Some layers from the model checkpoint at albert-base-v1 were not used when initializing TFAlbertForTokenClassification: ['predictions']\n",
            "- This IS expected if you are initializing TFAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFAlbertForTokenClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['dropout_4', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   Writing example 0 of 24000\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   *** Example ***\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   guid: train-1\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   tokens: [CLS] ▁sch art au ▁sag te ▁dem ▁ \" ▁tag es s pie gel ▁ \" ▁vo m ▁frei tag ▁ , ▁fischer ▁sei ▁ \" ▁in ▁ein er ▁we ise ▁auf get re ten ▁ , ▁die ▁all es ▁and ere ▁al s ▁ uber ze uge nd ▁war ▁ \" ▁ . [SEP]\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_ids: 2 4303 2900 1346 12644 591 11169 13 7 3383 160 18 9903 5436 13 7 6593 79 14532 8628 13 15 14445 8546 13 7 19 10235 106 95 2628 19301 3060 99 1316 13 15 1327 65 160 17 6345 493 18 13 8866 1734 13054 706 176 13 7 13 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   label_ids: -100 9 -100 -100 23 -100 23 23 -100 3 -100 -100 -100 -100 23 -100 23 -100 23 -100 23 -100 9 23 23 -100 23 23 -100 23 -100 23 -100 -100 -100 23 -100 23 23 -100 23 -100 23 -100 23 -100 -100 -100 -100 23 23 -100 23 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   *** Example ***\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   guid: train-2\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   tokens: [CLS] ▁firm eng r under ▁wolf ▁peter ▁bree ▁ arbeit ete ▁an fang ▁der ▁ sie b zig er ▁ jah re ▁al s ▁mob el vert re ter ▁ , ▁al s ▁ er ▁ein en ▁f lie gen den ▁handler ▁a us ▁dem ▁li ban on ▁tra f ▁ . [SEP]\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_ids: 2 1904 4367 139 4579 2597 936 18028 13 28270 11258 40 21682 2223 13 4157 220 16594 106 13 10671 99 493 18 7327 532 8122 99 815 13 15 493 18 13 106 10235 219 398 3844 1863 817 24641 21 267 11169 2093 4059 218 7957 410 13 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   label_ids: -100 23 -100 -100 -100 9 20 20 23 -100 -100 23 -100 23 23 -100 -100 -100 -100 23 -100 -100 23 -100 23 -100 -100 -100 -100 23 -100 23 -100 23 -100 23 -100 23 -100 -100 -100 23 23 -100 23 0 -100 -100 23 -100 23 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   *** Example ***\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   guid: train-3\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   tokens: [CLS] ▁ob ▁ sie ▁da bei ▁ nach ▁dem ▁run den ▁ t isch ▁am ▁23 . ▁april ▁in ▁berlin ▁dur ch ▁ein ▁pad agog ische s ▁kon ze pt ▁unter st utz t ▁wir d ▁ , ▁is t ▁all er ding s ▁zu ▁be z wei fel n ▁ . [SEP]\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_ids: 2 5122 13 4157 1331 11736 13 12468 11169 485 817 13 38 10435 589 1137 9 327 19 2296 6269 673 10235 4432 29997 9238 18 5191 1734 4417 19269 384 22017 38 12701 43 13 15 25 38 65 106 3258 18 6308 44 380 10515 6660 103 13 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   label_ids: -100 23 23 -100 23 -100 23 -100 23 23 -100 23 -100 -100 23 23 -100 23 23 0 23 -100 23 23 -100 -100 -100 23 -100 -100 23 -100 -100 -100 23 -100 23 -100 23 -100 23 -100 -100 -100 23 23 -100 -100 -100 -100 23 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   *** Example ***\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   guid: train-4\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   tokens: [CLS] ▁bayern ▁munchen ▁is t ▁wie der ▁all ein iger ▁top - ▁favor it ▁auf ▁den ▁ge win n ▁der ▁deutsche n ▁fu ß ball - meister schaft ▁ . [SEP]\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_ids: 2 22915 25601 25 38 11842 1157 65 12897 9038 371 8 3654 242 19301 5394 3168 4181 103 2223 11176 103 2916 1 3610 8 17271 25158 13 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   label_ids: -100 3 15 23 -100 23 -100 23 -100 -100 23 -100 23 -100 23 23 23 -100 -100 23 1 -100 23 -100 -100 -100 -100 -100 23 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   *** Example ***\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   guid: train-5\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   tokens: [CLS] ▁da bei ▁hat te ▁der ▁tap fer e ▁sch lus s mann ▁allen ▁grun d ▁ge hab t ▁ , ▁ s ich ▁vi el ▁fru her ▁auf zu re gen ▁ . [SEP]\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_ids: 2 1331 11736 2970 591 2223 5526 2407 62 4303 4334 18 1897 3675 14036 43 3168 9075 38 13 15 13 18 3870 1790 532 13394 1694 19301 4261 99 1863 13 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/07/2021 11:59:35 - INFO - utils_ner -   label_ids: -100 23 -100 23 -100 23 23 -100 -100 23 -100 -100 -100 23 23 -100 23 -100 -100 23 -100 23 -100 -100 23 -100 23 -100 23 -100 -100 -100 23 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/07/2021 11:59:46 - INFO - utils_ner -   Writing example 10000 of 24000\n",
            "03/07/2021 11:59:56 - INFO - utils_ner -   Writing example 20000 of 24000\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "2021-03-07 12:00:01.312736: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-07 12:00:01.315613: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_3271\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "2021-03-07 12:00:01.378801: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-03-07 12:00:01.379784: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFhz6rudaY6Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "b0695398-c72e-4f06-ce94-77914f83f4f2"
      },
      "source": [
        "!pip install transformers==3.4.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (4.41.1)\n",
            "Collecting tokenizers==0.9.2\n",
            "  Using cached https://files.pythonhosted.org/packages/35/e7/edf655ae34925aeaefb7b7fcc3dd0887d2a1203ee6b0df4d1170d1a19d4f/tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (20.9)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (0.1.91)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.4.0) (54.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.4.0) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.9.3\n",
            "    Uninstalling tokenizers-0.9.3:\n",
            "      Successfully uninstalled tokenizers-0.9.3\n",
            "  Found existing installation: transformers 3.5.0\n",
            "    Uninstalling transformers-3.5.0:\n",
            "      Successfully uninstalled transformers-3.5.0\n",
            "Successfully installed tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ufDNLXbi2qW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139dfbc4-ec26-4315-eef2-aeffbd0e168d"
      },
      "source": [
        "files=glob.glob('./abhilash1910/ner-model/*')\n",
        "files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./abhilash1910/ner-model/tf_model.h5',\n",
              " './abhilash1910/ner-model/config.json',\n",
              " './abhilash1910/ner-model/special_tokens_map.json',\n",
              " './abhilash1910/ner-model/test_results.txt',\n",
              " './abhilash1910/ner-model/tokenizer_config.json',\n",
              " './abhilash1910/ner-model/spiece.model',\n",
              " './abhilash1910/ner-model/test_predictions.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azDDv39ZqV0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ff99a2-d9f1-424e-c15b-77f595e37c8f"
      },
      "source": [
        "\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 21.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 25.9MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 29.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 4.6MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 4.0MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 4.0MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 4.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoRZChd_Rc57"
      },
      "source": [
        "## Training is completed\n",
        "\n",
        "After the training is done, we can move into evaluating the performance of the trained NER model by using the HF transformers pipeline. The pipeline is suited for faster inference on CPU and GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCVj0kupj213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed00fdeb-d47a-4c14-d4d6-ecbdbbd7a8d3"
      },
      "source": [
        "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer,TFAutoModelForTokenClassification\n",
        "\n",
        "\n",
        "ner_model = pipeline('ner', model='./abhilash1910/ner-model/', tokenizer='./abhilash1910/ner-model/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at ./abhilash1910/ner-model/ were not used when initializing TFAlbertModel: ['classifier', 'dropout_4']\n",
            "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFAlbertModel were not initialized from the model checkpoint at ./abhilash1910/ner-model/ and are newly initialized: ['pooler/kernel:0', 'pooler/bias:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some layers from the model checkpoint at ./abhilash1910/ner-model/ were not used when initializing TFAlbertForTokenClassification: ['dropout_4']\n",
            "- This IS expected if you are initializing TFAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFAlbertForTokenClassification were not initialized from the model checkpoint at ./abhilash1910/ner-model/ and are newly initialized: ['dropout_8']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18sQeThKkPgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc946c0-8875-4f04-9f4f-dbb4ecbc4f25"
      },
      "source": [
        "seq='Berlin ist die Hauptstadt von Deutschland'\n",
        "ner_model(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'B-ORGpart',\n",
              "  'index': 0,\n",
              "  'score': 0.08934617042541504,\n",
              "  'word': '[CLS]'},\n",
              " {'entity': 'B-PERderiv',\n",
              "  'index': 1,\n",
              "  'score': 0.09580112248659134,\n",
              "  'word': '▁berlin'},\n",
              " {'entity': 'B-ORGpart',\n",
              "  'index': 2,\n",
              "  'score': 0.08364498615264893,\n",
              "  'word': '▁is'},\n",
              " {'entity': 'B-LOCderiv',\n",
              "  'index': 3,\n",
              "  'score': 0.07593920826911926,\n",
              "  'word': 't'},\n",
              " {'entity': 'B-PERderiv',\n",
              "  'index': 4,\n",
              "  'score': 0.09574996680021286,\n",
              "  'word': '▁die'},\n",
              " {'entity': 'B-LOCderiv',\n",
              "  'index': 5,\n",
              "  'score': 0.07097965478897095,\n",
              "  'word': '▁'},\n",
              " {'entity': 'B-PERderiv',\n",
              "  'index': 6,\n",
              "  'score': 0.07122448086738586,\n",
              "  'word': 'haupt'},\n",
              " {'entity': 'B-PERderiv',\n",
              "  'index': 7,\n",
              "  'score': 0.12397754937410355,\n",
              "  'word': 'stadt'},\n",
              " {'entity': 'I-OTHderiv',\n",
              "  'index': 8,\n",
              "  'score': 0.0818650871515274,\n",
              "  'word': '▁von'},\n",
              " {'entity': 'I-LOCderiv',\n",
              "  'index': 9,\n",
              "  'score': 0.08271490037441254,\n",
              "  'word': '▁'},\n",
              " {'entity': 'B-LOCderiv',\n",
              "  'index': 10,\n",
              "  'score': 0.08616268634796143,\n",
              "  'word': 'deutschland'},\n",
              " {'entity': 'B-OTHpart',\n",
              "  'index': 11,\n",
              "  'score': 0.05508769303560257,\n",
              "  'word': '[SEP]'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOP9ndMzqW4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb257094-c6a0-42f5-a49e-184607b55468"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (3.5.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers) (54.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eZDCkbnqcTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515d314a-3f93-4e0a-8fdd-70e5d573d891"
      },
      "source": [
        "from transformers import AutoTokenizer,TFAutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "model=TFAutoModelForTokenClassification.from_pretrained('abhilash1910/albert-german-ner')\n",
        "tokenizer=AutoTokenizer.from_pretrained('abhilash1910/albert-german-ner')\n",
        "ner_model = pipeline('ner', model=model, tokenizer=tokenizer)\n",
        "seq='Berlin ist die Hauptstadt von Deutschland'\n",
        "ner_model(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at abhilash1910/albert-german-ner were not used when initializing TFAlbertForTokenClassification: ['dropout_4']\n",
            "- This IS expected if you are initializing TFAlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFAlbertForTokenClassification were not initialized from the model checkpoint at abhilash1910/albert-german-ner and are newly initialized: ['dropout_9']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'B-PERderiv',\n",
              "  'index': 1,\n",
              "  'score': 0.09580112248659134,\n",
              "  'word': '▁berlin'},\n",
              " {'entity': 'B-ORGpart',\n",
              "  'index': 2,\n",
              "  'score': 0.08364498615264893,\n",
              "  'word': '▁is'},\n",
              " {'entity': 'B-LOCderiv',\n",
              "  'index': 3,\n",
              "  'score': 0.07593920826911926,\n",
              "  'word': 't'},\n",
              " {'entity': 'B-PERderiv',\n",
              "  'index': 4,\n",
              "  'score': 0.09574996680021286,\n",
              "  'word': '▁die'},\n",
              " {'entity': 'B-LOCderiv',\n",
              "  'index': 5,\n",
              "  'score': 0.07097965478897095,\n",
              "  'word': '▁'},\n",
              " {'entity': 'B-PERderiv',\n",
              "  'index': 6,\n",
              "  'score': 0.07122448086738586,\n",
              "  'word': 'haupt'},\n",
              " {'entity': 'B-PERderiv',\n",
              "  'index': 7,\n",
              "  'score': 0.12397754937410355,\n",
              "  'word': 'stadt'},\n",
              " {'entity': 'I-OTHderiv',\n",
              "  'index': 8,\n",
              "  'score': 0.0818650871515274,\n",
              "  'word': '▁von'},\n",
              " {'entity': 'I-LOCderiv',\n",
              "  'index': 9,\n",
              "  'score': 0.08271490037441254,\n",
              "  'word': '▁'},\n",
              " {'entity': 'B-LOCderiv',\n",
              "  'index': 10,\n",
              "  'score': 0.08616268634796143,\n",
              "  'word': 'deutschland'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}